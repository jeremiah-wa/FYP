{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TripAdvisor dataset\n",
    "\n",
    "Retrieve csv file filtered from SQL relational tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "data = pd.read_csv(\"TripAdvisor.csv\", index_col=\"review_id\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Removing Duplicates\n",
    "- Ensure each user has rated at least 3 different hotels and each hotel has atleast 3 ratings\n",
    "- parsing html tags out of review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "\n",
    "data.drop_duplicates(subset=['hotel_id','member_id'], inplace=True)\n",
    "m = data.member_id.value_counts()\n",
    "df = data[data.member_id.isin(m[m>=5].index)]\n",
    "\n",
    "n = df.hotel_id.value_counts()\n",
    "df = df[df.hotel_id.isin(n[n>=3].index)]\n",
    "\n",
    "m = df.member_id.value_counts()\n",
    "df = df[df.member_id.isin(m[m>=3].index)]\n",
    "df  = df[~(df.hotel_id == 'St_Mark_Hotel')] # remove last remaining hotel with only 2 corresponding users\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Removing html tags\n",
    "def parse(x):\n",
    "    return re.sub('<[^<]+?>|&[^<]+?;','',x)\n",
    "\n",
    "parsed = list(map(parse,df.review_text))\n",
    "df = df.assign(review_text=parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print a quick review of a dataset\n",
    "def stats(df):\n",
    "    print('Users:',df['member_id'].value_counts().size)\n",
    "    print('Hotels:',df['hotel_id'].value_counts().size)\n",
    "    print('Reviews:',len(df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-item Matrix\n",
    "\n",
    "Analysis of user-item matrix created by TripAdvisor dataset (referenced in Report 7. Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating User-Item matrix\n",
    "matrix = df.pivot_table(index='member_id', columns='hotel_id', values='rating')\n",
    "print('User-Item Matrix:',matrix.shape[0],'x',matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Density\n",
    "\n",
    "Very low density level means less chance of overlap which will negatively affect neighbourhood size and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the Density of Matrix\n",
    "\n",
    "rated_hotels = sum(~np.isnan(matrix.values))\n",
    "density = sum(rated_hotels)/np.prod(matrix.shape)\n",
    "print('Density: ',round(density*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-to-user matrix\n",
    "\n",
    "Here I have created a user by user matrix with binary entries. 1 indicates a pair of users have co-rated items, 0 indicates there are no co-rated items. We can get an understanding of the overlaps between the users by computing the density of this matrix, however because, for a given user, I only care about the correlation with other users I will remove Mij where i=j, thus giving the percentage of correlated different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user = matrix.T.corr(method=lambda x,y:True)\n",
    "user_user.fillna(0, inplace=True)\n",
    "rated = sum(user_user.values)\n",
    "density = (sum(rated)-len(user_user))/(np.prod(user_user.shape)-len(user_user))\n",
    "print('Density: ',round(density*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-to-item matrix\n",
    "\n",
    "Similarly, the item-to-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item = matrix.corr(method=lambda x,y: True)\n",
    "item_item.fillna(0, inplace=True)\n",
    "rated = sum(item_item.values)\n",
    "density = (sum(rated)-len(item_item))/(np.prod(item_item.shape)-len(item_item))\n",
    "print('Density: ',round(density*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Reviews per user and hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# function for printing out the graphs of Number of Reviews per user/hotel\n",
    "def graph(df,column,step=5):\n",
    "    print('Total users:',df[column].value_counts().size)\n",
    "    print('Reviews count breakdown:')\n",
    "\n",
    "    x = df[column].value_counts()\n",
    "    n = max(x.value_counts().index)\n",
    "    plt.hist(x, bins=np.arange(1, n, step=1), weights=np.ones(len(x)) / len(x))\n",
    "    plt.xlabel('Number of Reviews')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(np.arange(0, n, step=step))\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall dataset graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(df,'member_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(df,'hotel_id',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# function for spliting dataset into Training, validation and test sets\n",
    "# df - dataset to be split\n",
    "# returns dataframes of training, validation and test sets\n",
    "def getDatasets(df):\n",
    "    count = 0\n",
    "    while True: #continue until a valid datasets made\n",
    "        train, test = train_test_split(df, test_size=0.2)\n",
    "        test, validation = train_test_split(test,test_size=0.5)\n",
    "        \n",
    "        validation = removeMissing(validation,train)\n",
    "        test = removeMissing(test,train)\n",
    "        \n",
    "        count+=1\n",
    "        #stop while when both validation and test set have enough candidates or after 100 attempts\n",
    "        if (len(validation)>0 and len(test)>0) or count>100:\n",
    "            break\n",
    "                \n",
    "    return train, validation, test\n",
    "\n",
    "# removes users/hotels from validation and test set not in train set\n",
    "def removeMissing(x, y):\n",
    "    missing_hotel = ~x[\"hotel_id\"].isin(y[\"hotel_id\"])  # hotels in set x but not in set y\n",
    "    missing_user = ~x[\"member_id\"].isin(y[\"member_id\"]) # users in set x but not in set y\n",
    "    new_x = x.loc[~missing_hotel] # remove hotels\n",
    "    new_x = new_x.loc[~missing_user] # remove users\n",
    "    return new_x # return new test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Training, Validation, Test set split\n",
    "\n",
    "- Traing set: ~80%\n",
    "- Validation set: ~10%\n",
    "- Test set: ~10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell used to perform dataset split\n",
    "\n",
    "train, validation, test = getDatasets(df)\n",
    "print('Dataset:',len(df))\n",
    "print(f'Train: {len(train)} ({round(len(train)/len(df),3)*100}%) \\nValidation: {len(validation)} ({round(len(validation)/len(df),3)*100}%) \\nTest: {len(test)} ({round(len(test)/len(df),3)*100}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out statistics for training, validation and test sets\n",
    "print('***train***')\n",
    "stats(train)\n",
    "print('*validation*')\n",
    "stats(validation)\n",
    "print('***test***')\n",
    "stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset incase a repeated test in needed\n",
    "train.to_csv('train.csv', header=True, index=True)\n",
    "validation.to_csv('validation.csv', header=True, index=True)\n",
    "test.to_csv('test.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Reviews per user and hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(train, 'member_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(train,'hotel_id',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(validation, 'member_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(validation,'hotel_id',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(test,'member_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph(test,'hotel_id',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "\n",
    "class cf_recommender:\n",
    "    \n",
    "    #initialize object with all parameters as null\n",
    "    def __init__(self):\n",
    "        self.neighbourhood_size = None\n",
    "        self.threshold = None\n",
    "        self.sig_weight = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.matrix = None\n",
    "        self.simMatrix = None\n",
    "        self.neighbours = {}\n",
    "    \n",
    "    # function for creating user-item matrix and similarity matrix used\n",
    "    # for neighbourhood generation and prediction\n",
    "    # df - data set used to train model\n",
    "    # measure - similarity measure to use\n",
    "    # ub - boolean variable if true user-based approach used else item-based model is used\n",
    "    # sig_weight - optional significance weighting (default = 0; no weighting)\n",
    "    def fit(self,df,measure,ub,sig_weight=0):\n",
    "        self.neighbours = {}\n",
    "        self.measure = measure #indicate sim metric to use\n",
    "        self.sig_weight = sig_weight #assign significance weighting (default = 0)\n",
    "        \n",
    "        # create user-item matrix filled with ratings\n",
    "        self.matrix = df.pivot_table(index=df.iloc(axis=1)[0].name,\n",
    "                                     columns=df.iloc(axis=1)[1].name,\n",
    "                                     values=df.iloc(axis=1)[2].name)\n",
    "        \n",
    "        # initialize min and max used for MSD similarity metric\n",
    "        self.max = self.matrix.max(skipna=True).max()\n",
    "        self.min = self.matrix.min(skipna=True).min()\n",
    "        \n",
    "        # ensure active profile is in index and target is in columns\n",
    "        if not ub:\n",
    "            self.matrix  = self.matrix.T\n",
    "        \n",
    "        # profile-profile matrix indicates profiles have common ratings\n",
    "        self.bool_matrix = self.matrix.T.corr(method=lambda x,y: True)\n",
    "        \n",
    "        # create similarity matrix\n",
    "        # scores will be filled at neighbourhood generation to reduce complexity\n",
    "        self.simMatrix = pd.DataFrame(index=self.matrix.index, columns=self.matrix.index)\n",
    "        for i in self.matrix.index:\n",
    "            self.simMatrix[i][i] = 1\n",
    "            \n",
    "    # function for making recommendation for a profiles\n",
    "    # active - user/item profiles to recommend\n",
    "    # n - number of recommendations to make (default = None; corresponds to as much as possible)\n",
    "    def recommend(self,active,n=None):\n",
    "        \n",
    "        recommend_list = []\n",
    "        # do not observe profiles rated by active profile \n",
    "        seen = self.matrix.loc[active][~np.isnan(self.matrix.loc[active])].index.tolist()\n",
    "        \n",
    "        # look at profiles rated by neighbours not already \n",
    "        for profile in self.computeNeighbours(active):\n",
    "            neighbour_rated = self.matrix.loc[profile[0]][~np.isnan(self.matrix.loc[profile[0]])].index.tolist()\n",
    "            for r in neighbour_rated: \n",
    "                if r not in seen:\n",
    "                    seen.append(r)\n",
    "                    p = self.predict(active,r)\n",
    "                    if p >= 4:\n",
    "                        recommend_list.append((r,p))\n",
    "\n",
    "        return sorted(recommend_list, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # function for making overall rating predictio based on overall ratings\n",
    "    # active - user/item profiles to make prediction for\n",
    "    # target - item/user to predict a rating on\n",
    "    def predict(self,active, target):\n",
    "        mean = np.nanmean(self.matrix.loc[active].values)\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        \n",
    "        for neighbour in self.computeNeighbours(active):\n",
    "            rating = self.matrix.loc[neighbour[0]][target]\n",
    "            if ~np.isnan(rating):\n",
    "                top += neighbour[1]*(rating - mean)\n",
    "                bottom += neighbour[1]\n",
    "\n",
    "        if bottom > 0:\n",
    "            return round(float(mean + top/bottom),2)\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    # function to define neighbourhood  \n",
    "    # k determines size of neighbourhood (default = None)\n",
    "    # t is the threshold similarity score for the nieghbourhood (default = none)\n",
    "    def neighbourhood(self,k=None,t=None):\n",
    "        self.neighbourhood_size = k\n",
    "        self.threshold = t\n",
    "    \n",
    "    # Neighbourhood generation function\n",
    "    # computes neighbours for a given profile\n",
    "    # populates sim-matrix interatively\n",
    "    def computeNeighbours(self,profile1):\n",
    "        if profile1 in self.neighbours.keys(): # if neighbours for given profile has already calculated\n",
    "            if self.threshold is not None: # if threshold (t) is given\n",
    "                # return list of neighbours with sim score > threshold\n",
    "                return list(filter(lambda x: x[1] > self.threshold, self.neighbours[profile1])) \n",
    "            else:# if neighbourhood size (k) is given \n",
    "                return self.neighbours[profile1][:self.neighbourhood_size]# return list of k nearest neighbours\n",
    "        \n",
    "        data = []\n",
    "        # get profiles with common rating with active profile\n",
    "        for profile2 in self.bool_matrix[profile1].dropna().index: \n",
    "            if profile1 != profile2: # ignore profile if equal to active profile\n",
    "                \n",
    "                sim = self.computeSim(profile1,profile2)\n",
    "                self.simMatrix[profile1][profile2] = sim #add to similarity Matrix\n",
    "\n",
    "                if sim>0: # only consider profiles with positive similarity scores\n",
    "                    data.append((profile2,sim)) # added profile and sim score tuple to neigbourhood\n",
    "                \n",
    "        self.neighbours[profile1] = sorted(data, key=lambda x: x[1], reverse=True) # sort neighbour in descending order according to sim score\n",
    "            \n",
    "        if self.threshold is not None:\n",
    "            return list(filter(lambda x: x[1] > self.threshold, self.neighbours[profile1]))\n",
    "        else:\n",
    "            return self.neighbours[profile1][:self.neighbourhood_size]\n",
    "        \n",
    "    # function for computing similarity matrix based on specific measure\n",
    "    def computeSim(self,p1,p2):\n",
    "        if not np.isnan(self.simMatrix[p1][p2]):\n",
    "            return self.simMatrix[p1][p2]\n",
    "        \n",
    "        r1 = self.matrix.loc[p1].dropna() #get profiles rated by active profiles\n",
    "        r2 = self.matrix.loc[p2].dropna() #get profiles rated by other profiles\n",
    "        common = set(r1.index).intersection(r2.index)\n",
    "        if common == 0:\n",
    "            return 0\n",
    "        \n",
    "        if self.measure == 'cosine':\n",
    "            dot_product = sum(r1.loc[common]*r2.loc[common])\n",
    "            norm1 = np.linalg.norm(r1)\n",
    "            norm2 = np.linalg.norm(r2)\n",
    "\n",
    "            if norm1*norm2 > 0:\n",
    "                return dot_product/(norm1*norm2)\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        elif self.measure == 'pearson':\n",
    "            # if correlation is undefined catch warning and return -1\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('error')\n",
    "                try:\n",
    "                    p = pearsonr(r1.loc[common].values, r2.loc[common].values)[0] # correlation is undefined when there is zero variance between arrays\n",
    "                except RuntimeWarning:\n",
    "                    return -1 \n",
    "\n",
    "            if len(common)< self.sig_weight:\n",
    "                return p*(len(common)/self.sig_weight)\n",
    "            else:\n",
    "                return p\n",
    "            \n",
    "        elif self.measure == 'msd':\n",
    "            msd = sum((r1.loc[common]-r2.loc[common])**2)/len(common)\n",
    "            return (1 - msd/(self.max - self.min)**2)\n",
    "        else:\n",
    "            raise ValueError(\"Measure must be either msd,cosine or pearson.\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function\n",
    "Used to test the recommenders on the evaluation metrics, RMSE and Coverage over a given set. Used for validaton testing and final evaluation of MC and CF recommenders. Function also returns the average neighbourhood size for additional analysis\n",
    "\n",
    "Paramters: \n",
    "- recs - recommender object (cf / mc)\n",
    "- evalset - evaluate to be used set (valdiation/test set) \n",
    "- ub - boolean variable to identify the approach being used (user-based or item-based) \n",
    "\n",
    "Returns: \n",
    "- RMSE\n",
    "- Coverage\n",
    "- Average Neighbourhood size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate(rec,evalSet,ub):\n",
    "    rmse = float('nan')\n",
    "    coverage = 0\n",
    "    avg = 0\n",
    "    pred = []\n",
    "    actual = []\n",
    "    \n",
    "    for idx, row in evalSet.iterrows():\n",
    "        if ub:\n",
    "            p = rec.predict(row[0],row[1])\n",
    "        else:\n",
    "            p = rec.predict(row[1],row[0])\n",
    "            \n",
    "        if p > -1:\n",
    "            pred.append(p)\n",
    "            actual.append(row[2])\n",
    "            \n",
    "    coverage = len(pred)/len(evalSet['rating'])\n",
    "    if coverage>0:\n",
    "        rmse = math.sqrt(mean_squared_error(actual, pred))\n",
    "    \n",
    "    for n in rec.neighbours:\n",
    "        avg+=len(rec.computeNeighbours(n))\n",
    "    avg/=len(rec.neighbours)\n",
    "    \n",
    "    return round(rmse,4), round(coverage,4), round(avg,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Testing Functions\n",
    "\n",
    "The following functions are used for testing CF and MC recommmender with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for testing different k in K-nearset-neighbour (KNN) algorithm\n",
    "# recs - recommender object class\n",
    "# test - dataset to use for testing\n",
    "# ub - boolean to specify if which approach to use i.e. user-based or item-based\n",
    "# returns table of k, model (ub/ib), RMSE , Coverage and Neighbourhood size\n",
    "def test_knn(rec,test,ub):\n",
    "    print(\"*****Neighbourhood size test*****\")\n",
    "    cols = ['K' ,'Model' , 'RMSE', 'Coverage', 'Neighbourhood size']\n",
    "    result = []\n",
    "    print(cols)\n",
    "    for k in range(10,260,10):\n",
    "        rec.neighbourhood(k)\n",
    "        rmse, coverage,n = evaluate(rec,test,ub)\n",
    "        res = [k,'User-based' if ub else 'Item-based',rmse,coverage,n]\n",
    "        print(res)\n",
    "        result.append(res)\n",
    "    print('\\n')\n",
    "    return pd.DataFrame(result, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for testing different thresholds in threshold neighbourhood algorithm\n",
    "# recs - recommender object class\n",
    "# test - dataset to use for testing\n",
    "# ub - boolean to specify if which approach to use i.e. user-based (ub) or item-based (ib)\n",
    "# returns table of threshold, model (ub/ib), RMSE , Coverage and Neighbourhood size\n",
    "def test_threshold(rec,test,ub):\n",
    "    print(\"*****Threshold similarity test*****\")\n",
    "    cols = ['Threshold' ,'Model' , 'RMSE', 'Coverage', 'Neighbourhood size']\n",
    "    result = []\n",
    "    print(cols)\n",
    "    for t in range(0,100,10):\n",
    "        rec.neighbourhood(t=(t/100))\n",
    "        rmse, coverage,n = evaluate(rec,test,ub)\n",
    "        res = [t/100,'User-based' if ub else 'Item-based',rmse,coverage,n]\n",
    "        print(res)\n",
    "        result.append(res)\n",
    "    print('\\n')\n",
    "    return pd.DataFrame(result, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test significance weighting\n",
    "# recs - recommender object class\n",
    "# train - dataset to use for training\n",
    "# test - dataset to use for testing\n",
    "# ub - boolean to specify if which approach to use i.e. user-based (ub) or item-based (ib)\n",
    "# metric - similarity metric to use (default = pearson)\n",
    "# returns table of Significance weight, model (ub/ib), RMSE , Coverage and Neighbourhood size\n",
    "def test_sig(rec,train,test,ub,metric='pearson'):\n",
    "    print(\"*****Threshold similarity test*****\")\n",
    "    cols = ['Sig. Weight' ,'Model' , 'RMSE', 'Coverage', 'Neighbourhood size']\n",
    "    result = []\n",
    "    print(cols)\n",
    "    for sig in range(1,11):\n",
    "        rec.fit(train,metric,ub,sig)\n",
    "        rec.neighbourhood()\n",
    "        rmse, coverage,n = evaluate(rec,test,ub)\n",
    "        res = [sig,'User-based' if ub else 'Item-based',rmse,coverage,n]\n",
    "        print(res)\n",
    "        result.append(res)\n",
    "    print('\\n')\n",
    "    return pd.DataFrame(result, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print validation result to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_csv(res,cols,filename,metrics=['Pearson','Cosine','MSD']):\n",
    "    results = pd.concat([res[0][cols],res[1][cols],res[2][cols]], axis=1)\n",
    "    columns = []\n",
    "    for m in metrics:\n",
    "        for i in cols:\n",
    "            columns+= [m + ' ' +i]\n",
    "    results.columns = columns\n",
    "    results.index = res[0].iloc(axis=1)[0].values\n",
    "    results.to_csv(filename+'.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Testing - CF\n",
    "\n",
    "Conducted testing on Pearson, Cosine and MSD metrics using KNN and threshold neighbourhood generation algorithms. (referenced in Report Ch 9.1)\n",
    "\n",
    "### KNN algorithm\n",
    "\n",
    "##### User-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "cf_knnTests_ub = []\n",
    "for metric in metrics:\n",
    "    cf = cf_recommender()\n",
    "    cf.fit(train,metric,True)\n",
    "    cf_knnTests_ub.append(test_knn(cf,validation,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Item-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "cf_knnTests_ib = []\n",
    "for metric in metrics:\n",
    "    cf = cf_recommender()\n",
    "    cf.fit(train,metric,False)\n",
    "    cf_knnTests_ib.append(test_knn(cf,validation,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold neighbourhood algorithm\n",
    "\n",
    "#### User-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "cf_thresholdTests_ub = []\n",
    "for metric in metrics:\n",
    "    cf = cf_recommender()\n",
    "    cf.fit(train,metric,True)\n",
    "    cf_thresholdTests_ub.append(test_threshold(cf,validation,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "cf_thresholdTests_ib = []\n",
    "for metric in metrics:\n",
    "    cf = cf_recommender()\n",
    "    cf.fit(train,metric,False)\n",
    "    cf_thresholdTests_ib.append(test_threshold(cf,validation,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_csv(cf_knnTests_ub,['RMSE','Coverage'],'cf_knn_ub')\n",
    "result_to_csv(cf_knnTests_ib,['RMSE','Coverage'],'cf_knn_ib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_csv(cf_thresholdTests_ub,['RMSE','Coverage','Neighbourhood size'],'cf_threshold_ub')\n",
    "result_to_csv(cf_thresholdTests_ib,['RMSE','Coverage','Neighbourhood size'],'cf_threshold_ib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Significance weighting\n",
    "\n",
    "Additionally I tested the effects of significance weighting on a user-based CF model using the Pearson metric and print the results to a corresponding csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = cf_recommender()\n",
    "cf_sigweightTest = test_sig(cf,train,validation,True)\n",
    "cf_sigweightTest.to_csv('cf_sigTest.csv',header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-criteria recommender (MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "\n",
    "\n",
    "class mc_recommender:\n",
    "    \n",
    "    #initialize class with all parameters as null\n",
    "    def __init__(self):\n",
    "        self.sig_weight = None\n",
    "        self.measure = None\n",
    "        self.neighbourhood_size = None\n",
    "        self.threshold = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.matrix = None\n",
    "        self.index = None\n",
    "        self.simMatrix = None\n",
    "        self.bool_matrix = None\n",
    "        self.neighbours = None\n",
    "        self.df = None\n",
    "    \n",
    "    # function for creating matrices used for preforming multi-criteria recommendations\n",
    "    # df - dataframe used for training model\n",
    "    # measure - string specifing what similarity measure to use\n",
    "    # ub - if True similarity calculation is user-based, else item-based\n",
    "    # sig_weight - optional signifincance weighting (default = 0)\n",
    "    def fit(self,df,measure,ub,sig_weight=0):\n",
    "        self.measure = measure # set similarity measure to be used in sim() function\n",
    "        self.neighbours = {} # wipe old neighbourhood\n",
    "        self.neighbourhood_size = None\n",
    "        self.threshold = None\n",
    "        self.sig_weight = sig_weight\n",
    "        \n",
    "        # create user-item matrix for used creating profile-to-profile matrix\n",
    "        self.matrix = df.pivot_table(index=df.iloc(axis=1)[0].name,\n",
    "                           columns=df.iloc(axis=1)[1].name,\n",
    "                           values=df.iloc(axis=1)[2].name)\n",
    "        \n",
    "        # retrieve min and max rating to use for MSD similarity\n",
    "        self.max = self.matrix.max(skipna=True).max()\n",
    "        self.min = self.matrix.min(skipna=True).min()\n",
    "        \n",
    "        # compute profile-to-profile matrix used to identify profiles with common ratings\n",
    "        # ub boolean determines which matrix to use i.e. user-based or item-based\n",
    "        if ub:\n",
    "            self.bool_matrix = self.matrix.T.corr(method=lambda x,y: True) # compute user-to-user matrix\n",
    "            self.index = 0 # indicate user profile is the profile to use in computing similarity\n",
    "        else:\n",
    "            self.bool_matrix = self.matrix.corr(method=lambda x,y: True)# compute item-to-item matrix\n",
    "            self.matrix = self.matrix.T\n",
    "            self.index = 1            \n",
    "        \n",
    "        # similarity matrix\n",
    "        # sim scores will be filled in during neighbourhood generation to reduce time complexity\n",
    "        self.simMatrix = pd.DataFrame(index=self.matrix.index, columns=self.matrix.index)\n",
    "        for i in self.matrix.index:\n",
    "            self.simMatrix[i][i] = 1\n",
    "        \n",
    "        #compute 3D user-item-criteria-matrix\n",
    "        data = []\n",
    "        for idx, row in df.iterrows():\n",
    "            rlist = [i.split(':') for i in [sub.split(';') for sub in [row[3]]][0]]\n",
    "            rdict = {i[1]:float(i[0]) for i in rlist}\n",
    "            data.append(rdict)\n",
    "            \n",
    "        #3D matrix represented by multi-index dataframe (index = (user,item), columns = criteria ratings)\n",
    "        self.df = pd.DataFrame(data,index=[df.iloc(axis=1)[0], df.iloc(axis=1)[1]])\n",
    "        \n",
    "        \n",
    "    # function for configuring parameters of neighbourhood generation function\n",
    "    # k - specifies the neighbourhood size\n",
    "    # t -  specifies the threshold similarity\n",
    "    def neighbourhood(self,k=None,t=None):\n",
    "        self.neighbourhood_size = k\n",
    "        self.threshold = t\n",
    "            \n",
    "    # function for generating neighbourhood for given profile\n",
    "    # using multi-criteria ratings and paramters specified by neighbourhood()\n",
    "    # this function also fills similarity matrix\n",
    "    def computeNeighbours(self,profile1):\n",
    "            \n",
    "        if profile1 in self.neighbours.keys(): # if neighbours for given profile has already calculated\n",
    "            if self.threshold is not None: # if threshold (t) is given\n",
    "                # return list of neighbours with sim score > threshold\n",
    "                return list(filter(lambda x: x[1] > self.threshold, self.neighbours[profile1])) \n",
    "            else:# if neighbourhood size (k) is given \n",
    "                return self.neighbours[profile1][:self.neighbourhood_size]# return list of k nearest neighbours\n",
    "        \n",
    "        data = []\n",
    "        for profile2 in self.bool_matrix.loc[self.bool_matrix[profile1]==True].index: #get profiles with co-rating to active profile\n",
    "            if profile1 != profile2: # ignore profile if equal to active profile\n",
    "                \n",
    "                sim = self.computeSim(profile1,profile2)\n",
    "\n",
    "                if sim>0: # only consider profiles with positive similarity scores\n",
    "                    data.append((profile2,sim)) # added profile and sim score tuple to neigbourhood\n",
    "                    \n",
    "        # store all possible neigbours\n",
    "        self.neighbours[profile1] = sorted(data, key=lambda x: x[1], reverse=True) # sort neighbour in descending order according to sim score\n",
    "        \n",
    "        # return the neighbours specified\n",
    "        if self.threshold is not None:\n",
    "            return list(filter(lambda x: x[1] > self.threshold, self.neighbours[profile1]))\n",
    "        else:\n",
    "            return self.neighbours[profile1][:self.neighbourhood_size]\n",
    "    \n",
    "    # function for calculating similarity score between two profiles\n",
    "    # updates simMatrix iterably\n",
    "    def computeSim(self,profile1,profile2):\n",
    "        if not np.isnan(self.simMatrix[profile1][profile2]):\n",
    "            return self.simMatrix[profile1][profile2]\n",
    "        \n",
    "        r1 = self.df.xs(profile1, level=self.index) #get profiles rated by active profiles\n",
    "        r2 = self.df.xs(profile2, level=self.index) #get profiles rated by other profiles\n",
    "        common = list(set(r1.index)&set(r2.index))   #get co-rated profiles\n",
    "        if len(common) == 0:\n",
    "            self.simMatrix[profile1][profile2] = 0\n",
    "            return 0\n",
    "        \n",
    "        scores = []\n",
    "        for idx in common: # for each co-rated profile\n",
    "            c1 = set(np.where(~np.isnan(r1.loc[idx]))[0].tolist()) # get criteria (sub-rating) rated by active profile\n",
    "            c2 = set(np.where(~np.isnan(r2.loc[idx]))[0].tolist()) # get criteria rated by other profile\n",
    "            intersect = len(list(c1 & c2)) # get number of overlapping criteria\n",
    "            if intersect >0: # if there is at least 1 co-rated criterion\n",
    "                scores.append(self.sim(r1.loc[idx][c1],r2.loc[idx][c2]))\n",
    "\n",
    "        # get mean criteria similarity score of all co-rated profiles\n",
    "        # if no co-rated criteria, let overall sim score be 0\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            try:\n",
    "                sim = np.nanmean(scores) # throws warning if array of sim is all NaNs or empty\n",
    "            except RuntimeWarning:\n",
    "                sim = 0\n",
    "            \n",
    "            # if sig_weighted selcted compute significance weighting\n",
    "            if len(common)< self.sig_weight:\n",
    "                sim*=(len(common)/self.sig_weight)\n",
    "        \n",
    "        self.simMatrix[profile1][profile2] = sim #add to similarity Matrix\n",
    "        return sim\n",
    "    \n",
    "    # function for computing sim score using criteria ratings\n",
    "    # similarity measure used is specified by the fit() function\n",
    "    # input - criteria rating arrays\n",
    "    def sim(self,r1,r2):\n",
    "        common = set(r1.index).intersection(r2.index)\n",
    "        if common == 0:\n",
    "            return 0\n",
    "        \n",
    "        if self.measure == 'cosine':\n",
    "            dot_product = sum(r1[common]*r2[common])\n",
    "            norm1 = np.linalg.norm(r1)\n",
    "            norm2 = np.linalg.norm(r2)\n",
    "            \n",
    "            if norm1*norm2 > 0:\n",
    "                return dot_product/(norm1*norm2)\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        elif self.measure == 'pearson':\n",
    "            # if correlation is undefined catch warning and return -1\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('error')\n",
    "                try:\n",
    "                    # correlation is undefined when there is zero variance between arrays\n",
    "                    # or when one common rating between profiles\n",
    "                    p = pearsonr(r1[common].values, r2[common].values)[0] \n",
    "                except RuntimeWarning:\n",
    "                    return -1\n",
    "                \n",
    "                return p\n",
    "            \n",
    "        elif self.measure == 'msd':\n",
    "            msd = sum((r1[common]-r2[common])**2)/len(common)\n",
    "            return (1 - msd/(self.max - self.min)**2)\n",
    "        else:\n",
    "            raise ValueError(\"Measure must be either msd,cosine or pearson.\")\n",
    "    \n",
    "    \n",
    "    # function for predicting the overall rating a given (active) profile would give a target profile\n",
    "    # returns a float ranging from 0 to 5, or -1 if no prediction could be made\n",
    "    def predict(self,active, target):\n",
    "        \n",
    "        # get the mean rating of the active profile\n",
    "        mean = np.nanmean(self.matrix.loc[active])\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        \n",
    "        #Resnick's Prediction Algorithm (deviation from mean approach)\n",
    "        #for each profile if neighbourhood of active profile\n",
    "        for neighbour in self.computeNeighbours(active):\n",
    "            rating = self.matrix.loc[neighbour[0]][target]\n",
    "            if ~np.isnan(rating): # only consider neighbours that have rated the target profile\n",
    "                top += neighbour[1]*(rating - mean)\n",
    "                bottom += neighbour[1]\n",
    "                \n",
    "        # if a pool of neighbours who have rated the target user was found return prediction\n",
    "        # else return -1 to indicate failure to predict\n",
    "        if bottom > 0: \n",
    "            return round(float(mean + top/bottom),2)\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    # function for recommending profiles for a given (active) profile\n",
    "    # based on the active profiles prediction rating for profiles rated by neighbours\n",
    "    # the function returns a list of the top n recommendations (profile,predicted rating) tuples (default n = None; as much as possible)   \n",
    "    def recommend(self,active,n=None):\n",
    "            \n",
    "        rated = self.df.xs(active, level=self.index).index.tolist() # retrieve list of profiles rated by active profile\n",
    "        recommend_list = []\n",
    "        seen = []\n",
    "        seen += rated # add rated profiles to list of observed profiles\n",
    "        \n",
    "        # for each profile in active profile neighbourhoood\n",
    "        for profile in self.computeNeighbours(active):\n",
    "            neighbour_rated = self.df.xs(profile[0], level=self.index).index.tolist() # get profiles rated by neighbour\n",
    "            for r in neighbour_rated: # for each profile rated by neighbour\n",
    "                #if not already observed, perform a prediction using the prediction function\n",
    "                if r not in seen:\n",
    "                    seen.append(r) # add to observed list\n",
    "                    p = self.predict(active,r)\n",
    "                    #if the prediction is >= threshold rating (4) add to recommendation list\n",
    "                    if p >= 4:\n",
    "                        recommend_list.append((r,p))\n",
    "                   \n",
    "        # return a list of top n recommendations\n",
    "        return sorted(recommend_list, key=lambda x: x[1], reverse=True)[:n]  \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Testing - MC\n",
    "\n",
    "### KNN algorithm\n",
    "\n",
    "#### User-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "mc_knnTests_ub = []\n",
    "for metric in metrics:\n",
    "    mc = mc_recommender()\n",
    "    mc.fit(train,metric,True)\n",
    "    mc_knnTests_ub.append(test_knn(mc,validation,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "mc_knnTests_ib = []\n",
    "for metric in metrics:\n",
    "    mc = mc_recommender()\n",
    "    mc.fit(train,metric,False)\n",
    "    mc_knnTests_ib.append(test_knn(mc,validation,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold neighbourhood algorithm\n",
    "\n",
    "#### User-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "mc_thresholdTests_ub = []\n",
    "for metric in metrics:\n",
    "    mc = mc_recommender()\n",
    "    mc.fit(train,metric,True)\n",
    "    mc_thresholdTests_ub.append(test_threshold(mc,validation,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Item-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['pearson','cosine','msd']\n",
    "mc_thresholdTests_ib = []\n",
    "for metric in metrics:\n",
    "    mc = mc_recommender()\n",
    "    mc.fit(train,metric,False)\n",
    "    mc_thresholdTests_ib.append(test_threshold(mc,validation,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_csv(mc_knnTests_ub,['RMSE','Coverage'],'mc_knnTests_ub')\n",
    "result_to_csv(mc_knnTests_ib,['RMSE','Coverage'],'mc_knnTests_ib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_csv(mc_thresholdTests_ub,['RMSE','Coverage','Neighbourhood size'],'mc_thresholdTests_ub')\n",
    "result_to_csv(mc_thresholdTests_ib,['RMSE','Coverage','Neighbourhood size'],'mc_thresholdTests_ib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance weighting \n",
    "\n",
    "Additionally test the effect of significance weighting on MC approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = mc_recommender()\n",
    "mc_sigweightTest = test_sig(mc,train,validation,True, 'msd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_sigweightTest.to_csv('mc_sigTest.csv',header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - CF vs MC\n",
    "\n",
    "Compare the highest performing algorithms from both CF and MC approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = cf_recommender()\n",
    "mc = mc_recommender()\n",
    "\n",
    "cf.fit(train,'msd',True)\n",
    "mc.fit(train,'msd',True)\n",
    "cf.neighbourhood(k=150)\n",
    "mc.neighbourhood(k=150)\n",
    "\n",
    "results = []\n",
    "results.append(list(evaluate(cf,test,True)))\n",
    "results.append(list(evaluate(mc,test,True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns = ['RMSE','Coverage', 'Avg.Neighbourhood size'], index=['CF', 'MC']).to_csv('cf_mc_evaluation.csv',header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based (CB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "class cb_recommender:\n",
    "    \n",
    "    # function intialises an object of this class\n",
    "    def __init__(self):\n",
    "        self.index = None # specify whether recommendations are made for users or items\n",
    "        self.data  = None # training dataset\n",
    "        self.df = None # transformed training set: reviews grouped by users or item\n",
    "        self.neighbours = None # similar users/items\n",
    "        self.tfidf = None # keyword feature matrix\n",
    "        self.simMatrix = None # similarity matrix\n",
    "        \n",
    "    # function for creating matrices used for preforming multi-criteria recommendations\n",
    "    # data - dataframe used for training model\n",
    "    # ub - if True similarity calculation is user-based, else item-based\n",
    "    def fit(self,data, ub=None):\n",
    "        self.index = int(ub)\n",
    "        self.data = data\n",
    "        df_by = pd.DataFrame(data.reset_index(drop=True).groupby(data.columns[int(ub)])['review_text'])\n",
    "\n",
    "        all_reviews = []\n",
    "        for idx in df_by.index:\n",
    "            reviews = str()\n",
    "            for review in df_by.loc[idx][1]:\n",
    "                reviews += ' ' +review\n",
    "            all_reviews.append(reviews)\n",
    "        \n",
    "        self.__computeSimMatrix(all_reviews)\n",
    "        \n",
    "        data = {'id': df_by[0],'review_text': all_reviews}\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.__computeNeighbours()\n",
    "            \n",
    "    #function to get top n similar profiles to given (target) profile\n",
    "    # target - given profile  to find similar profiles for\n",
    "    # n - number of similar profiles to give (default = 10)\n",
    "    # returns list of tuples (profile, similarity score)\n",
    "    def similarTo(self,target,n=10):\n",
    "        return self.neighbours[target][:n]\n",
    "    \n",
    "    # function for recommending a items to users or users to items\n",
    "    # active - the profile to recommend\n",
    "    # n - the number of recommendations (Default = None; as much as possible)\n",
    "    # t - option for threshold of similarity required for a recommendation (default = 0; so long as theres any positive similarity)\n",
    "    # returns list of recommendation tuples (profile, similarity score)\n",
    "    def recommend(self,active,n=None,t=0):\n",
    "        observed = self.data[self.data.iloc(axis=1)[abs(self.index - 1)]==active].iloc(axis=1)[self.index].tolist()\n",
    "        recommend_list = []\n",
    "        seen = []\n",
    "        seen += observed\n",
    "        \n",
    "        # for each profile observed by active profile\n",
    "        # search similar profiles and add to recommendation list if not yet observed by active profile\n",
    "        # and if similarity score to observed profile is greater than some threshold (default t = 0)\n",
    "        for i in observed:\n",
    "            for profile in self.neighbours[i][:100]:#default only look at top 100 neighbours\n",
    "                \n",
    "                #if profile has a higher score replace the one in recommendation list\n",
    "                if profile[0] in seen: \n",
    "                    for p in recommend_list:\n",
    "                        if p[0] == profile[0] and p[1] < profile[1]:\n",
    "                            recommend_list.remove(p)\n",
    "                            recommend_list.append(profile)\n",
    "                            break\n",
    "                    continue\n",
    "                    \n",
    "                # if profile has a similarity greater than threshold add to recommendation list\n",
    "                if profile[1] > t:\n",
    "                    seen.append(profile[0])\n",
    "                    recommend_list.append(profile)\n",
    "                \n",
    "        return sorted(recommend_list, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # function for neighbourhood generation\n",
    "    # selects profiles from the grouped reviews dataframe and \n",
    "    # forms neighbours based on similarity from the similarity matrix\n",
    "    def __computeNeighbours(self):\n",
    "        neighbours = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            similar_indices = np.argsort(-1*self.simMatrix[idx]) #rank profile according to similarity score\n",
    "            similar = [(self.df['id'][i],self.simMatrix[idx][i]) for i in similar_indices]\n",
    "            neighbours[row['id']] = similar[1:] # remove current active profile from neighbourhood\n",
    "        self.neighbours = neighbours # initialize neighbourhood\n",
    "    \n",
    "    # function for computing the similarity matrix\n",
    "    # creates vector space model using tf-idf and \n",
    "    # computes similarity between rows based on the cosine similarity measure\n",
    "    # text - array of reviews by profile\n",
    "    def __computeSimMatrix(self,text):\n",
    "        self.tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "        tfidf_matrix = self.tfidf.fit_transform(text)\n",
    "        # vectors are normalised so cosine is given by computing linear kernel (dot product)\n",
    "        self.simMatrix = linear_kernel(tfidf_matrix , tfidf_matrix) \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating recommendations\n",
    "# This function will only look at profiles in test set that have had ratings >= 4\n",
    "# recommend - a recommender's recommend function\n",
    "# test - set to be tested with\n",
    "# ub - boolean to specify whether users or items are to be recommended\n",
    "# returns a dictionary of the recommendations where key=profile and value=recommendations tuple\n",
    "def getRecs(recommend,test,ub):\n",
    "    truthset = test.loc[test.rating>=4].sort_values(by=['rating'], ascending=False)\n",
    "    profiles  = truthset.iloc(axis=1)[abs(ub - 1)].unique().tolist()\n",
    "    recommend_list = {}\n",
    "    \n",
    "    for profile in profiles:\n",
    "        recommend_list[profile] = recommend(profile)\n",
    "#         print(profile)\n",
    "        \n",
    "    return recommend_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn recommendations into a table in the dataframe \n",
    "# dictionary of recommendations\n",
    "# returns dataframe of recommendation\n",
    "def recs_to_df(recs):\n",
    "    return pd.DataFrame(list(map(lambda x: recs[x], recs.keys())), index=recs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb =  cb_recommender()\n",
    "cb.fit(train, True)\n",
    "cb_recs = getRecs(cb.recommend, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf =  cf_recommender()\n",
    "cf.fit(train,'msd',True)\n",
    "cf.neighbourhood(150)\n",
    "cf_recs = getRecs(cf.recommend, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc =  mc_recommender()\n",
    "mc.fit(train,'msd',True)\n",
    "mc.neighbourhood(150)\n",
    "mc_recs = getRecs(mc.recommend, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print recommendations to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_to_df(cb_recs).to_csv('cb_recs.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_to_df(cf_recs).to_csv('cf_recs.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_to_df(cf_recs).to_csv('mc_recs.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation -  CF vs MC vs CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Precision (AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function computes the average precision of N recommendations for a profile\n",
    "# profile - user/item profile being recommended\n",
    "# recs - dictionary of all recommendation\n",
    "# n - number of recommendation to look at\n",
    "# ub - indicates if profile is a user or item profile\n",
    "#\n",
    "# returns\n",
    "# precs - list of precision score up to n recommendations\n",
    "# recalls - list of recalls score up to n recommendations\n",
    "# average precision up to n recommendations\n",
    "def averagePrecision(profile,recs,n,ub):\n",
    "    truthset = test.loc[test.rating>=4].sort_values(by=['rating'], ascending=False)\n",
    "    t = truthset.loc[truthset.iloc(axis=1)[abs(ub - 1)]==profile].iloc(axis=1)[int(ub)].values.tolist()\n",
    "    r = list(map(lambda x: x[0], recs[profile]))[:n]\n",
    "    \n",
    "    precs = []\n",
    "    recalls = []\n",
    "    recoms = [1 if i in t else 0 for i in r]\n",
    "#     print(recoms)\n",
    "    for idx, rec in enumerate(recoms):\n",
    "        precs.append(sum(recoms[:idx+1])/(idx+1))\n",
    "        recalls.append(sum(recoms[:idx+1])/len(t))\n",
    "    \n",
    "    return precs, recalls , (1/len(t))*sum(precs[:n])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision vs Recall Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "profile = test.member_id.value_counts().index[0]\n",
    "n = 150\n",
    "recalls = [None] * 3\n",
    "precs = [None] * 3\n",
    "precs[0], recalls[0], _ = averagePrecision(profile,cb_recs,n,True)\n",
    "precs[1], recalls[1], _ = averagePrecision(profile,cf_recs,n,True)\n",
    "precs[2], recalls[2], _ = averagePrecision(profile,mc_recs,n,True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(recalls[0], precs[0], color='blue')\n",
    "line2, = ax.plot(recalls[1], precs[1], color='orange')\n",
    "line3, = ax.plot(recalls[2], precs[2], color='grey')\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"P(i) vs. r(i) for Increasing $i$ for AP@\"+str(n))\n",
    "ax.legend((line1, line2, line3), ('CB', 'CF', 'MC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function computes the mean average precision of every profile being recommended\n",
    "# recs - dictionary of recommendations\n",
    "# n - number of recommendations to look at\n",
    "# ub - indicates if profile is a user or item profile\n",
    "#\n",
    "# return mean average precision\n",
    "def meanAvgPrec(recs,n,ub):\n",
    "    avgPrecs = []\n",
    "    for profile in recs.keys():\n",
    "        _,_,ap = averagePrecision(profile,recs,n,ub)\n",
    "        avgPrecs.append(ap)\n",
    "        \n",
    "    return (1/len(recs))*sum(avgPrecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP of CF, MC and CB Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the mean average precision for CB, CF and MC recommendations\n",
    "# print of mean average precision to mean_avg_precision.csv\n",
    "\n",
    "scores = []\n",
    "for recs in [cb_recs,cf_recs,mc_recs]:\n",
    "    score = []\n",
    "    for n in range(10,310,10):\n",
    "        score.append(meanAvgPrec(recs,n,True))\n",
    "    scores.append(score)\n",
    "    \n",
    "pd.DataFrame(scores,index=['CB','CF','MC'], columns= [i for i in range(10,310,10)]).T.to_csv('mean_avg_precision.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute diversity of N recommendations\n",
    "# profile - given (user/item) profile\n",
    "# n - number of recommendations\n",
    "# recs - given recommendations\n",
    "# simMatrix - similarity matrix for recommendation profiles\n",
    "# returns diversity score\n",
    "def diversity(profile,n,recs,simMatrix):\n",
    "    top = 0\n",
    "    recoms = recs[profile][:n]\n",
    "    num = len(recoms)\n",
    "    for i in recoms:\n",
    "        for j in recoms:\n",
    "            if i[0] == j[0]:\n",
    "                continue\n",
    "            sim = simMatrix[i[0]][j[0]]\n",
    "            top+= (1-sim) if not np.isnan(sim) else 1\n",
    "            \n",
    "    return top/(num*(num-1)) if n>1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average diversity over all profiles recommended\n",
    "# n - number of recommendations\n",
    "# recs - given recommendations\n",
    "# simMatrix - similarity matrix for recommendation profiles\n",
    "# ub - specify user-based/item-based\n",
    "def avgDiversity(n,recs,simMatrix, ub):\n",
    "    div = 0\n",
    "    for profile in recs.keys():\n",
    "        div+= diversity(profile,n,recs,simMatrix)\n",
    "    \n",
    "    return div/len(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Diversity for different number of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average diversity for different number of recommendations\n",
    "# recs - recommendations to use\n",
    "# simMatrix - similarity of recommended profiles\n",
    "# ub - specify user-based/item-based\n",
    "def getDiv(recs,simMatrix,ub):\n",
    "    scores = []\n",
    "    for n in range(10,110,10):\n",
    "        div = avgDiversity(n,recs,simMatrix,ub)\n",
    "        scores.append(div)\n",
    "#         print(n,',',div)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average Diversity of CF. MC and CB recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list for accumalation all the diversity scores\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = cb_recs\n",
    "simMatrix = pd.DataFrame(cb.simMatrix, index=cb.df['id'], columns=cb.df['id'])\n",
    "scores.append(getDiv(recs,simMatrix,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is necessary to compute an item-item similarity matrix \n",
    "# in order to compute the diversity in the recommendations\n",
    "cf_a = cf_recommender()\n",
    "cf_a.fit(train,'msd',False)\n",
    "cf_a.neighbourhood()\n",
    "for items in np.unique(test.hotel_id.values):\n",
    "    cf_a.computeNeighbours(items)\n",
    "\n",
    "scores.append(getDiv(cf_recs,cf_a.simMatrix,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is necessary to compute an item-item similarity matrix \n",
    "# in order to compute the diversity in the recommendations\n",
    "mc_a = mc_recommender()\n",
    "mc_a.fit(train,'msd',False)\n",
    "mc_a.neighbourhood()\n",
    "for items in np.unique(test.hotel_id.values):\n",
    "    mc_a.computeNeighbours(items)\n",
    "    \n",
    "scores.append(getDiv(mc_recs,mc_a.simMatrix,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results to csv\n",
    "pd.DataFrame(scores, columns=range(0,110,10), index=['CB','CF','MC']).T.to_csv('avg_diversity.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coverage of recommendations\n",
    "def getCoverage(recs,ub):\n",
    "    truthset = test.loc[test.rating>3].sort_values(by=['rating'], ascending=False)\n",
    "    profiles  = truthset.iloc(axis=1)[abs(ub - 1)].unique().tolist()\n",
    "    l = [1 if len(recs[p])>0 else 0 for p in profiles]\n",
    "    return sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph coverage for CF, MC and CB recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print graph of coverage\n",
    "x = np.arange(3)\n",
    "plt.bar(x,[getCoverage(cb_recs,True),getCoverage(cf_recs,True),getCoverage(mc_recs,True)])\n",
    "plt.xticks(x, ('CB', 'CF', 'MC'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
